{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version is: 2.3.1\n",
      "numpy version is: 2.3.1\n",
      "sklearn version is 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import swifter\n",
    "import sklearn\n",
    "\n",
    "print(f\"pandas version is: {pd.__version__}\\nnumpy version is: {np.__version__}\\nsklearn version is {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_groups(filepath: str, min_members: int = 200, encoding: str = \"utf-8-sig\", errors: str = \"replace\", low_memory: bool = False):\n",
    "    \"\"\"\n",
    "    Load the dataset and filter out groups with no name or low member count\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df[df[\"peer_name\"].notnull()]\n",
    "    df = df[df[\"participants\"] > min_members]\n",
    "    df = df.drop_duplicates(subset=\"peerid\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean a given text by removing links, phone numbers, mentions, emojis, symbols, and non-Persian/English characters\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove links\n",
    "    text = re.sub(r'http\\S+|www\\S+|t\\.me/\\S+|telegram\\.me/\\S+', '', text)\n",
    "\n",
    "    # Remove phone numbers\n",
    "    text = re.sub(r'(\\+98|0098|0)?9\\d{9}', '', text)\n",
    "\n",
    "    # Remove @mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"       \n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002700-\\U000027BF\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Remove symbols and punctuation (keep Persian and English characters)\n",
    "    text = re.sub(r'[^\\w\\s\\u0600-\\u06FFa-zA-Z]', ' ', text)\n",
    "\n",
    "    # Remove meaningless short words\n",
    "    meaningful_short_words = {'Ø¢Ø¨', 'ÙÙ†', 'Ù…Ù‡', 'Ù„Ø¨'}\n",
    "    text = ' '.join([w for w in text.split() if len(w) >= 3 or w in meaningful_short_words])\n",
    "\n",
    "    # Normalize Arabic characters to Persian\n",
    "    text = text.replace('ÙŠ', 'ÛŒ').replace('Ùƒ', 'Ú©')\n",
    "\n",
    "    # Keep only Persian and English words\n",
    "    text = ' '.join(re.findall(r'[a-zA-Z\\u0600-\\u06FF]+', text))\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def is_persian_dominant(text, threshold: float = 0.3):\n",
    "    \"\"\"\n",
    "    Check if the majority of the text contains Persian characters (based on a threshold)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return False\n",
    "    total_chars = len(re.findall(r'\\w', text))\n",
    "    persian_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n",
    "    if total_chars == 0:\n",
    "        return False\n",
    "    return (persian_chars / total_chars) >= threshold\n",
    "\n",
    "\n",
    "def is_really_persian(text):\n",
    "    \"\"\"\n",
    "    Return True if the text contains Persian-specific letters\n",
    "    and does not contain Arabic-specific characters\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return False\n",
    "\n",
    "    # Persian-specific letters\n",
    "    persian_letters = \"Ù¾Ú†Ú˜Ú¯Ú©ÛŒÛŒ\"\n",
    "\n",
    "    # Arabic-specific letters that should NOT be present\n",
    "    arabic_only_letters = \"Ø©Ù‰ÙŠï»»Ù€\"\n",
    "\n",
    "    # Must contain at least one Persian letter\n",
    "    has_persian = any(char in text for char in persian_letters)\n",
    "    # Must not contain any Arabic-specific characters\n",
    "    has_arabic_only = any(char in text for char in arabic_only_letters)\n",
    "\n",
    "    return has_persian and not has_arabic_only\n",
    "\n",
    "def clean_group_data(df):\n",
    "    df['name_clean'] = df['peer_name'].apply(clean_text)\n",
    "    df['about_clean'] = df['about'].apply(clean_text)\n",
    "\n",
    "    # Keep only rows with real Persian (not Arabic) content\n",
    "    df = df[df['name_clean'].apply(is_really_persian) | df['about_clean'].apply(is_really_persian)]\n",
    "\n",
    "    # Drop empty or NaN\n",
    "    df = df.dropna(subset=['name_clean', 'about_clean'])\n",
    "    df = df[(df['name_clean'].str.strip() != \"\") & (df['about_clean'].str.strip() != \"\")]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "filepath = \"/mnt/d/Uni/thesis/data/idekav_subscription.csv\"\n",
    "df = load_and_filter_groups(filepath)\n",
    "df_clean = clean_group_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ÙØ§ÛŒÙ„ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø´Ø®ØµØ§ØªØ´ÙˆÙ† Ø±Ùˆ Ø¯Ø§Ø±ÛŒÙ… (ØªÙ…ÛŒØ² Ø´Ø¯Ù‡)\n",
    "groups_df  = pd.read_csv(\"../data/processed/groups_clean.csv\")\n",
    "\n",
    "# ÙØ§ÛŒÙ„ Ø¹Ø¶ÙˆÛŒØª (Ù…Ø«Ù„Ø§Ù‹ Ø´Ø§Ù…Ù„ user_id Ùˆ group_id)\n",
    "membership_df  = pd.read_csv(\"/mnt/d/Uni/thesis/data/newsubtact.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_group_ids = set(groups_df[\"peerid\"])\n",
    "filtered_membership = membership_df[membership_df[\"groupID\"].isin(valid_group_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ Ù‚Ø¨Ù„ Ø§Ø² ÙÛŒÙ„ØªØ±: 658037\n",
      "ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ Ø¨Ø¹Ø¯ Ø§Ø² ÙÛŒÙ„ØªØ±: 201249\n",
      "Ú†Ù†Ø¯ Ú¯Ø±ÙˆÙ‡ Ø­Ø°Ù Ø´Ø¯Ù†Ø¯ØŸ 456788\n"
     ]
    }
   ],
   "source": [
    "total_before = membership_df[\"groupID\"].nunique()\n",
    "total_after = filtered_membership[\"groupID\"].nunique()\n",
    "print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ Ù‚Ø¨Ù„ Ø§Ø² ÙÛŒÙ„ØªØ±: {total_before}\")\n",
    "print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ Ø¨Ø¹Ø¯ Ø§Ø² ÙÛŒÙ„ØªØ±: {total_after}\")\n",
    "print(f\"Ú†Ù†Ø¯ Ú¯Ø±ÙˆÙ‡ Ø­Ø°Ù Ø´Ø¯Ù†Ø¯ØŸ {total_before - total_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_membership.to_csv(\"../data/processed/membership_filtered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df =pd.read_csv(\"/mnt/d/Uni/thesis/dissertation/telegram_community_detection/data/processed/groups_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/membership_filtered.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "group_encoder = LabelEncoder()\n",
    "user_encoder = LabelEncoder()\n",
    "\n",
    "group_ids = group_encoder.fit_transform(df[\"groupID\"])\n",
    "user_ids = user_encoder.fit_transform(df[\"userID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "rows = user_ids\n",
    "cols = group_ids\n",
    "data = [1] * len(df)\n",
    "\n",
    "membership_matrix = coo_matrix((data, (rows, cols)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_overlap = membership_matrix.T @ membership_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_groupID = group_encoder.inverse_transform(np.arange(len(group_encoder.classes_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# min_common_members = 5\n",
    "# group_overlap.setdiag(0)  # Ø­Ø°Ù self-loops\n",
    "\n",
    "# coo = group_overlap.tocoo()\n",
    "# edges = [\n",
    "#     (group_encoder.inverse_transform([i])[0],  # real groupID i\n",
    "#      group_encoder.inverse_transform([j])[0],  # real groupID j\n",
    "#      w)\n",
    "#     for i, j, w in zip(coo.row, coo.col, coo.data)\n",
    "#     if w >= min_common_members\n",
    "# ]\n",
    "import numpy as np\n",
    "\n",
    "min_common_members = 5\n",
    "group_overlap.setdiag(0)  # Ø­Ø°Ù self-loops\n",
    "\n",
    "coo = group_overlap.tocoo()\n",
    "\n",
    "mask = coo.data >= min_common_members\n",
    "rows = coo.row[mask]\n",
    "cols = coo.col[mask]\n",
    "weights = coo.data[mask]\n",
    "\n",
    "edges = [\n",
    "    (int(index_to_groupID[i]), int(index_to_groupID[j]), int(w))\n",
    "    for i, j, w in zip(rows, cols, weights)\n",
    "    if i < j  # Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù ÛŒØ§Ù„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ú†ÙˆÙ† Ú¯Ø±Ø§Ù Ø¨ÛŒâ€ŒØ¬Ù‡Øª Ù‡Ø³Øª\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±Ù‡â€ŒÙ‡Ø§: 92679\n",
      "ðŸ“Œ ØªØ¹Ø¯Ø§Ø¯ ÛŒØ§Ù„â€ŒÙ‡Ø§: 6629782\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "print(f\"ðŸ“Œ ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±Ù‡â€ŒÙ‡Ø§: {G.number_of_nodes()}\")\n",
    "print(f\"ðŸ“Œ ØªØ¹Ø¯Ø§Ø¯ ÛŒØ§Ù„â€ŒÙ‡Ø§: {G.number_of_edges()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
